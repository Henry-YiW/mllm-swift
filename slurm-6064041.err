Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:12,  6.17s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:12<00:06,  6.11s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.25s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.49s/it]
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:17<28:54, 17.52s/it]  2%|▏         | 2/100 [00:33<27:29, 16.83s/it]  3%|▎         | 3/100 [00:51<28:08, 17.41s/it]  4%|▍         | 4/100 [01:09<27:54, 17.45s/it]  5%|▌         | 5/100 [01:25<26:59, 17.04s/it]  6%|▌         | 6/100 [01:42<26:27, 16.89s/it]  7%|▋         | 7/100 [02:00<26:59, 17.42s/it]  8%|▊         | 8/100 [02:17<26:26, 17.24s/it]  9%|▉         | 9/100 [02:34<25:47, 17.01s/it] 10%|█         | 10/100 [02:52<26:15, 17.51s/it] 11%|█         | 11/100 [03:09<25:38, 17.29s/it] 12%|█▏        | 12/100 [03:26<25:21, 17.29s/it] 13%|█▎        | 13/100 [03:43<24:32, 16.93s/it] 14%|█▍        | 14/100 [03:59<24:08, 16.85s/it] 15%|█▌        | 15/100 [04:16<23:54, 16.87s/it] 16%|█▌        | 16/100 [04:33<23:32, 16.82s/it] 17%|█▋        | 17/100 [04:50<23:25, 16.94s/it] 18%|█▊        | 18/100 [05:07<23:03, 16.87s/it] 19%|█▉        | 19/100 [05:23<22:40, 16.80s/it] 20%|██        | 20/100 [05:41<22:48, 17.10s/it] 21%|██        | 21/100 [05:58<22:15, 16.90s/it] 22%|██▏       | 22/100 [06:14<21:41, 16.69s/it] 23%|██▎       | 23/100 [06:31<21:24, 16.69s/it] 24%|██▍       | 24/100 [06:48<21:25, 16.91s/it] 25%|██▌       | 25/100 [07:05<21:21, 17.09s/it] 26%|██▌       | 26/100 [07:22<20:50, 16.90s/it] 27%|██▋       | 27/100 [07:40<20:55, 17.19s/it] 28%|██▊       | 28/100 [07:57<20:28, 17.06s/it] 29%|██▉       | 29/100 [08:14<20:11, 17.06s/it] 30%|███       | 30/100 [08:30<19:46, 16.95s/it] 31%|███       | 31/100 [08:47<19:15, 16.74s/it] 32%|███▏      | 32/100 [09:03<18:57, 16.72s/it] 33%|███▎      | 33/100 [09:19<18:24, 16.48s/it] 34%|███▍      | 34/100 [09:37<18:29, 16.81s/it] 35%|███▌      | 35/100 [09:55<18:37, 17.19s/it] 36%|███▌      | 36/100 [10:12<18:10, 17.04s/it] 37%|███▋      | 37/100 [10:29<18:10, 17.32s/it] 38%|███▊      | 38/100 [10:46<17:44, 17.17s/it] 39%|███▉      | 39/100 [11:04<17:38, 17.35s/it] 40%|████      | 40/100 [11:23<17:56, 17.95s/it] 41%|████      | 41/100 [11:41<17:39, 17.95s/it] 42%|████▏     | 42/100 [11:59<17:09, 17.74s/it] 43%|████▎     | 43/100 [12:16<16:44, 17.63s/it] 44%|████▍     | 44/100 [12:32<16:02, 17.19s/it] 45%|████▌     | 45/100 [12:49<15:40, 17.10s/it] 46%|████▌     | 46/100 [13:07<15:36, 17.35s/it] 47%|████▋     | 47/100 [13:25<15:33, 17.61s/it] 48%|████▊     | 48/100 [13:41<14:52, 17.17s/it] 49%|████▉     | 49/100 [13:58<14:33, 17.13s/it] 50%|█████     | 50/100 [14:17<14:39, 17.59s/it] 51%|█████     | 51/100 [14:35<14:20, 17.57s/it] 52%|█████▏    | 52/100 [14:51<13:46, 17.22s/it] 53%|█████▎    | 53/100 [15:08<13:23, 17.09s/it] 54%|█████▍    | 54/100 [15:28<13:52, 18.09s/it] 55%|█████▌    | 55/100 [15:45<13:11, 17.59s/it] 56%|█████▌    | 56/100 [16:02<12:45, 17.39s/it] 57%|█████▋    | 57/100 [16:20<12:37, 17.62s/it] 58%|█████▊    | 58/100 [16:36<12:02, 17.21s/it] 59%|█████▉    | 59/100 [16:53<11:44, 17.18s/it] 60%|██████    | 60/100 [17:10<11:25, 17.13s/it] 61%|██████    | 61/100 [17:27<11:06, 17.08s/it] 62%|██████▏   | 62/100 [17:44<10:45, 16.98s/it] 63%|██████▎   | 63/100 [18:01<10:28, 17.00s/it] 64%|██████▍   | 64/100 [18:17<10:04, 16.78s/it] 65%|██████▌   | 65/100 [18:34<09:44, 16.69s/it] 66%|██████▌   | 66/100 [18:51<09:31, 16.81s/it] 67%|██████▋   | 67/100 [19:08<09:21, 17.02s/it] 68%|██████▊   | 68/100 [19:25<08:58, 16.82s/it] 69%|██████▉   | 69/100 [19:41<08:34, 16.61s/it] 70%|███████   | 70/100 [19:57<08:16, 16.54s/it] 71%|███████   | 71/100 [20:15<08:14, 17.04s/it] 72%|███████▏  | 72/100 [20:31<07:50, 16.81s/it] 73%|███████▎  | 73/100 [20:49<07:39, 17.03s/it] 74%|███████▍  | 74/100 [21:07<07:26, 17.18s/it] 75%|███████▌  | 75/100 [21:23<07:05, 17.03s/it] 76%|███████▌  | 76/100 [21:41<06:55, 17.33s/it] 77%|███████▋  | 77/100 [21:57<06:30, 16.99s/it] 78%|███████▊  | 78/100 [22:14<06:10, 16.82s/it] 79%|███████▉  | 79/100 [22:31<05:54, 16.90s/it] 80%|████████  | 80/100 [22:48<05:40, 17.01s/it] 81%|████████  | 81/100 [23:04<05:16, 16.66s/it] 82%|████████▏ | 82/100 [23:21<05:03, 16.87s/it] 83%|████████▎ | 83/100 [23:38<04:44, 16.73s/it] 84%|████████▍ | 84/100 [23:54<04:24, 16.51s/it] 85%|████████▌ | 85/100 [24:11<04:08, 16.58s/it] 86%|████████▌ | 86/100 [24:28<03:54, 16.78s/it] 87%|████████▋ | 87/100 [24:45<03:41, 17.00s/it] 88%|████████▊ | 88/100 [25:03<03:25, 17.09s/it] 89%|████████▉ | 89/100 [25:19<03:05, 16.84s/it] 90%|█████████ | 90/100 [25:35<02:46, 16.64s/it] 91%|█████████ | 91/100 [25:51<02:29, 16.57s/it] 92%|█████████▏| 92/100 [26:09<02:14, 16.79s/it] 93%|█████████▎| 93/100 [26:26<01:58, 16.87s/it] 94%|█████████▍| 94/100 [26:43<01:41, 16.86s/it] 95%|█████████▌| 95/100 [27:00<01:25, 17.12s/it] 96%|█████████▌| 96/100 [27:18<01:08, 17.16s/it] 97%|█████████▋| 97/100 [27:35<00:51, 17.25s/it] 98%|█████████▊| 98/100 [27:53<00:35, 17.53s/it] 99%|█████████▉| 99/100 [28:10<00:17, 17.22s/it]100%|██████████| 100/100 [28:28<00:00, 17.55s/it]100%|██████████| 100/100 [28:28<00:00, 17.09s/it]
Instantiating LlamaAttention without passing a `layer_idx` is not recommended and will lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` when creating this class.
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.32s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.27s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.86s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.98s/it]
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/u/hyi1/.conda/envs/huggingface/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/u/hyi1/.conda/envs/huggingface/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/projects/bdpp/hyi1/prospective/SWIFT/evaluation_llama/inference_swift.py", line 303, in <module>
    run_eval(
  File "/projects/bdpp/hyi1/prospective/SWIFT/evaluation_llama/eval.py", line 96, in run_eval
    get_answers_func(
  File "/u/hyi1/.conda/envs/huggingface/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/projects/bdpp/hyi1/prospective/SWIFT/evaluation_llama/eval.py", line 142, in get_model_answers
    output_ids, new_token_num, step, accept_length_tree, draft_token_num = forward_func(
  File "/projects/bdpp/hyi1/prospective/SWIFT/evaluation_llama/inference_swift.py", line 39, in swift_forward
    swift_logits, sample_token, top1_prob = initialize_swift(input_ids, model, max_new_tokens,
  File "/projects/bdpp/hyi1/prospective/SWIFT/model/swift/utils.py", line 317, in initialize_swift
    outputs, logits = swift_verify(model, input_ids, past_key_values=past_key_values)
  File "/projects/bdpp/hyi1/prospective/SWIFT/model/swift/utils.py", line 360, in swift_verify
    outputs = model.model(
  File "/u/hyi1/.conda/envs/huggingface/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/u/hyi1/.conda/envs/huggingface/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/bdpp/hyi1/prospective/SWIFT/model/swift/modeling_llama.py", line 491, in forward
    layer_outputs = decoder_layer(
  File "/u/hyi1/.conda/envs/huggingface/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/u/hyi1/.conda/envs/huggingface/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/bdpp/hyi1/prospective/SWIFT/model/swift/modeling_llama.py", line 300, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/u/hyi1/.conda/envs/huggingface/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/u/hyi1/.conda/envs/huggingface/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/bdpp/hyi1/prospective/SWIFT/model/swift/modeling_llama.py", line 165, in forward
    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
  File "/u/hyi1/.conda/envs/huggingface/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/u/hyi1/.conda/envs/huggingface/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/u/hyi1/.conda/envs/huggingface/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
TypeError: LlamaRotaryEmbedding.forward() got an unexpected keyword argument 'seq_len'
srun: error: gpua096: task 0: Exited with exit code 1
